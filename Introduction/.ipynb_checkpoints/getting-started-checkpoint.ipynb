{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "absolute-macintosh",
   "metadata": {},
   "source": [
    "# Spark Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-gathering",
   "metadata": {},
   "source": [
    "Apache Spark is a unified analytics engine for large-scale data processing. It provides high-level APIs in Java, Scala, Python and R, and an optimized engine that supports general execution graphs. It also supports a rich set of higher-level tools including Spark SQL for SQL and structured data processing, MLlib for machine learning, GraphX for graph processing, and Structured Streaming for incremental computation and stream processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aerial-prediction",
   "metadata": {},
   "source": [
    "Main features of Apache Spark:\n",
    "- Apache Spark achieves high performance for both batch and streaming data, using a state-of-the-art DAG scheduler, a query optimizer, and a physical execution engine\n",
    "- Spark offers over 80 high-level operators that make it easy to build parallel apps. And you can use it interactively from the Scala, Python, R, and SQL shells\n",
    "- It powers a stack of libraries including SQL and DataFrames, MLlib for machine learning, GraphX, and Spark Streaming. You can combine these libraries seamlessly in the same application\n",
    "- It runs on Hadoop, Apache Mesos, Kubernetes, standalone, or in the cloud. It can access diverse data sources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tired-cleveland",
   "metadata": {},
   "source": [
    "## Pyspark\n",
    "Pyspark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fantastic-corporation",
   "metadata": {},
   "source": [
    "PySpark supports most of Spark's features such as \n",
    "- __Spark SQL and DataFrame__ : Spark SQL is a Spark module for structured data processing. It provides a programming abstraction called DataFrame and can also act as distributed SQL query engine\n",
    "- __Streaming__ : Running on top of Spark, the streaming feature in Apache Spark enables powerful interactive and analytical applications across both streaming and historical data, while inheriting Sparkâ€™s ease of use and fault tolerance characteristics\n",
    "- __MLib__ : Built on top of Spark, MLlib is a scalable machine learning library that provides a uniform set of high-level APIs that help users create and tune practical machine learning pipelines\n",
    "- __Spark Core__ : Spark Core is the underlying general execution engine for the Spark platform that all other functionality is built on top of. It provides an RDD (Resilient Distributed Dataset) and in-memory computing capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "driving-excess",
   "metadata": {},
   "source": [
    "PySpark DataFrames are lazily evaluated. They are implemented on top of RDDs. When Spark transforms data, it does not immediately compute the transformation but plans how to compute later. When actions such as collect() are explicitly called, the computation starts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "comparative-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing pyspark library and establishing Spark session which is the entry point pf PySpark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "moving-option",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>ID</th><th>age</th><th>occupation</th></tr>\n",
       "<tr><td>1</td><td>25</td><td>Data Scientist</td></tr>\n",
       "<tr><td>2</td><td>16</td><td>Singer</td></tr>\n",
       "<tr><td>3</td><td>21</td><td>CEO</td></tr>\n",
       "<tr><td>4</td><td>22</td><td>Doctor</td></tr>\n",
       "<tr><td>5</td><td>26</td><td>Singer</td></tr>\n",
       "<tr><td>6</td><td>21</td><td>CEO</td></tr>\n",
       "<tr><td>7</td><td>20</td><td>Researcher</td></tr>\n",
       "<tr><td>8</td><td>19</td><td>Doctor</td></tr>\n",
       "<tr><td>9</td><td>29</td><td>Journalist</td></tr>\n",
       "<tr><td>10</td><td>21</td><td>Journalist</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+--------------+\n",
       "| ID|age|    occupation|\n",
       "+---+---+--------------+\n",
       "|  1| 25|Data Scientist|\n",
       "|  2| 16|        Singer|\n",
       "|  3| 21|           CEO|\n",
       "|  4| 22|        Doctor|\n",
       "|  5| 26|        Singer|\n",
       "|  6| 21|           CEO|\n",
       "|  7| 20|    Researcher|\n",
       "|  8| 19|        Doctor|\n",
       "|  9| 29|    Journalist|\n",
       "| 10| 21|    Journalist|\n",
       "+---+---+--------------+"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a dataframe \n",
    "import pandas as pd\n",
    "from pyspark.sql import Row\n",
    "\n",
    "df = spark.createDataFrame([\n",
    "    Row(ID=1, age=25, occupation = 'Data Scientist'),\n",
    "    Row(ID=2, age=16, occupation = 'Singer'),\n",
    "    Row(ID=3, age=21, occupation = 'CEO'),\n",
    "    Row(ID=4, age=22, occupation = 'Doctor'),\n",
    "    Row(ID=5, age=26, occupation = 'Singer'),\n",
    "    Row(ID=6, age=21, occupation = 'CEO'),\n",
    "    Row(ID=7, age=20, occupation = 'Researcher'),\n",
    "    Row(ID=8, age=19, occupation = 'Doctor'),\n",
    "    Row(ID=9, age=29, occupation = 'Journalist'),\n",
    "    Row(ID=10, age=21, occupation = 'Journalist')\n",
    "    \n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorrect-mechanics",
   "metadata": {},
   "source": [
    "Resilient Distributed Datasets (RDD) is a fundamental data structure of PySpark, It is an immutable distributed collection of objects. Each dataset in RDD is divided into logical partitions, which may be computed on different nodes of the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acquired-college",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>ID</th><th>age</th><th>occupation</th></tr>\n",
       "<tr><td>1</td><td>25</td><td>Data Scientist</td></tr>\n",
       "<tr><td>2</td><td>16</td><td>Singer</td></tr>\n",
       "<tr><td>3</td><td>21</td><td>CEO</td></tr>\n",
       "<tr><td>4</td><td>22</td><td>Doctor</td></tr>\n",
       "<tr><td>5</td><td>26</td><td>Singer</td></tr>\n",
       "<tr><td>6</td><td>21</td><td>CEO</td></tr>\n",
       "<tr><td>7</td><td>20</td><td>Researcher</td></tr>\n",
       "<tr><td>8</td><td>19</td><td>Doctor</td></tr>\n",
       "<tr><td>9</td><td>29</td><td>Journalist</td></tr>\n",
       "<tr><td>10</td><td>21</td><td>Journalist</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "+---+---+--------------+\n",
       "| ID|age|    occupation|\n",
       "+---+---+--------------+\n",
       "|  1| 25|Data Scientist|\n",
       "|  2| 16|        Singer|\n",
       "|  3| 21|           CEO|\n",
       "|  4| 22|        Doctor|\n",
       "|  5| 26|        Singer|\n",
       "|  6| 21|           CEO|\n",
       "|  7| 20|    Researcher|\n",
       "|  8| 19|        Doctor|\n",
       "|  9| 29|    Journalist|\n",
       "| 10| 21|    Journalist|\n",
       "+---+---+--------------+"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Creating a pyspark dataframe from an RDD consisting of a list of tuples\n",
    "rdd = spark.sparkContext.parallelize([\n",
    "    Row(ID=1, age=25, occupation = 'Data Scientist'),\n",
    "    Row(ID=2, age=16, occupation = 'Singer'),\n",
    "    Row(ID=3, age=21, occupation = 'CEO'),\n",
    "    Row(ID=4, age=22, occupation = 'Doctor'),\n",
    "    Row(ID=5, age=26, occupation = 'Singer'),\n",
    "    Row(ID=6, age=21, occupation = 'CEO'),\n",
    "    Row(ID=7, age=20, occupation = 'Researcher'),\n",
    "    Row(ID=8, age=19, occupation = 'Doctor'),\n",
    "    Row(ID=9, age=29, occupation = 'Journalist'),\n",
    "    Row(ID=10, age=21, occupation = 'Journalist')\n",
    "])\n",
    "data = spark.createDataFrame(rdd)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "super-horizontal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(ID=1, age=25, occupation='Data Scientist'),\n",
       " Row(ID=2, age=16, occupation='Singer'),\n",
       " Row(ID=3, age=21, occupation='CEO'),\n",
       " Row(ID=4, age=22, occupation='Doctor'),\n",
       " Row(ID=5, age=26, occupation='Singer'),\n",
       " Row(ID=6, age=21, occupation='CEO'),\n",
       " Row(ID=7, age=20, occupation='Researcher'),\n",
       " Row(ID=8, age=19, occupation='Doctor'),\n",
       " Row(ID=9, age=29, occupation='Journalist'),\n",
       " Row(ID=10, age=21, occupation='Journalist')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Collects the distributed data to the driver side as the local data in python\n",
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "italian-maria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "environmental-liechtenstein",
   "metadata": {},
   "source": [
    "__Viewing data present in dataframes__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "equal-trinity",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------+\n",
      "| ID|age|    occupation|\n",
      "+---+---+--------------+\n",
      "|  1| 25|Data Scientist|\n",
      "|  2| 16|        Singer|\n",
      "|  3| 21|           CEO|\n",
      "|  4| 22|        Doctor|\n",
      "|  5| 26|        Singer|\n",
      "|  6| 21|           CEO|\n",
      "|  7| 20|    Researcher|\n",
      "|  8| 19|        Doctor|\n",
      "|  9| 29|    Journalist|\n",
      "| 10| 21|    Journalist|\n",
      "+---+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "completed-theology",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      " |-- occupation: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "distinct-operation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>ID</th><th>age</th><th>occupation</th></tr>\n",
       "<tr><td>1</td><td>25</td><td>Data Scientist</td></tr>\n",
       "<tr><td>2</td><td>16</td><td>Singer</td></tr>\n",
       "<tr><td>3</td><td>21</td><td>CEO</td></tr>\n",
       "<tr><td>4</td><td>22</td><td>Doctor</td></tr>\n",
       "<tr><td>5</td><td>26</td><td>Singer</td></tr>\n",
       "<tr><td>6</td><td>21</td><td>CEO</td></tr>\n",
       "<tr><td>7</td><td>20</td><td>Researcher</td></tr>\n",
       "<tr><td>8</td><td>19</td><td>Doctor</td></tr>\n",
       "<tr><td>9</td><td>29</td><td>Journalist</td></tr>\n",
       "<tr><td>10</td><td>21</td><td>Journalist</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[ID: bigint, age: bigint, occupation: string]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Enabling Eager evaluation mode of pyspark dataframe\n",
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "limited-poultry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|              age|\n",
      "+-------+-----------------+\n",
      "|  count|               10|\n",
      "|   mean|             22.0|\n",
      "| stddev|3.741657386773941|\n",
      "|    min|               16|\n",
      "|    max|               29|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.select(\"age\").describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "temporal-strengthening",
   "metadata": {},
   "source": [
    "__Selecting and Accessing Data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "naughty-parcel",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'age'>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "undefined-likelihood",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "|age|\n",
      "+---+\n",
      "| 25|\n",
      "| 16|\n",
      "| 21|\n",
      "| 22|\n",
      "| 26|\n",
      "| 21|\n",
      "| 20|\n",
      "| 19|\n",
      "| 29|\n",
      "| 21|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Select takes the column instances and returns another dataframe\n",
    "data.select(data.age).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "interpreted-desire",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------+----------------+\n",
      "| ID|age|    occupation|Upper_occupation|\n",
      "+---+---+--------------+----------------+\n",
      "|  1| 25|Data Scientist|  DATA SCIENTIST|\n",
      "|  2| 16|        Singer|          SINGER|\n",
      "|  3| 21|           CEO|             CEO|\n",
      "|  4| 22|        Doctor|          DOCTOR|\n",
      "|  5| 26|        Singer|          SINGER|\n",
      "|  6| 21|           CEO|             CEO|\n",
      "|  7| 20|    Researcher|      RESEARCHER|\n",
      "|  8| 19|        Doctor|          DOCTOR|\n",
      "|  9| 29|    Journalist|      JOURNALIST|\n",
      "| 10| 21|    Journalist|      JOURNALIST|\n",
      "+---+---+--------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Assigning new column instance\n",
    "from pyspark.sql.functions import upper\n",
    "data.withColumn('Upper_occupation', upper(data.occupation)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "mediterranean-discrimination",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+----------+\n",
      "| ID|age|occupation|\n",
      "+---+---+----------+\n",
      "|  3| 21|       CEO|\n",
      "|  6| 21|       CEO|\n",
      "| 10| 21|Journalist|\n",
      "+---+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Selecting a subset of rows\n",
    "data.filter(data.age == 21).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "swiss-custody",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-------+--------+\n",
      "|    occupation|avg(ID)|avg(age)|\n",
      "+--------------+-------+--------+\n",
      "|        Singer|    3.5|    21.0|\n",
      "|           CEO|    4.5|    21.0|\n",
      "|Data Scientist|    1.0|    25.0|\n",
      "|    Researcher|    7.0|    20.0|\n",
      "|    Journalist|    9.5|    25.0|\n",
      "|        Doctor|    6.0|    20.5|\n",
      "+--------------+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.groupby('occupation').avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closing-ultimate",
   "metadata": {},
   "source": [
    "__Getting data in/out__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-logging",
   "metadata": {},
   "source": [
    "CSV is straightforward and easy to use. Parquet and ORC are efficient and compact file formats to read and write faster. There are many other data sources available in PySpark such as JDBC, text, binaryFile, Avro, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alpine-skiing",
   "metadata": {},
   "source": [
    "JSON format:\n",
    "\n",
    "{\"id\": 1, \"first_name\": \"Matthew\", \"last_name\": \"Rathbone\", \"age\": 19, \"cool\": true, \"favorite_fruit\": [\"bananas\", \"apples\"]}\n",
    "{\"id\": 2, \"first_name\": \"Joe\", \"last_name\": \"Bloggs\", \"age\": 102, \"cool\": true, \"favorite_fruit\": null}\n",
    "\n",
    "This is the format of the columnar format:\n",
    "\n",
    "Field Name/Field Type/Number of Characters:[data in csv format]\n",
    "\n",
    "ID/INT/3:1,2\n",
    "FIRST_NAME/STRING/11:Matthew,Joe\n",
    "LAST_NAME/STRING/15:Rathbone,Bloggs\n",
    "AGE/INT/6:19,102\n",
    "COOL/BOOL/3:1,1\n",
    "FAVORITE_FRUIT/ARRAY[STRING]/19:[bananas,apples],[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interpreted-satellite",
   "metadata": {},
   "source": [
    "__Working with SQL__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heated-pleasure",
   "metadata": {},
   "source": [
    "DataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. For example, you can register the DataFrame as a table and run a SQL easily as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "listed-dance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+--------------+\n",
      "| ID|age|    occupation|\n",
      "+---+---+--------------+\n",
      "|  1| 25|Data Scientist|\n",
      "|  2| 16|        Singer|\n",
      "|  3| 21|           CEO|\n",
      "|  4| 22|        Doctor|\n",
      "|  5| 26|        Singer|\n",
      "|  6| 21|           CEO|\n",
      "|  7| 20|    Researcher|\n",
      "|  8| 19|        Doctor|\n",
      "|  9| 29|    Journalist|\n",
      "| 10| 21|    Journalist|\n",
      "+---+---+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.createOrReplaceTempView(\"tableA\")\n",
    "spark.sql(\"SELECT * from tableA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crude-monthly",
   "metadata": {},
   "source": [
    "These SQL expressions can directly be mixed and used as PySpark columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "floppy-niger",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|(count(1) > 0)|\n",
      "+--------------+\n",
      "|          true|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "data.select(expr('count(*)') > 0).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
