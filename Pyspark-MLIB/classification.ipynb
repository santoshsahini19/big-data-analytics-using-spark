{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "exclusive-fortune",
   "metadata": {},
   "source": [
    "# Classification in PySpark's MLlib\n",
    "\n",
    "PySpark offers a good variety of algorithms that can be applied to classification machine learning problems. However, because PySpark operates on distributed dataframes, we cannot use popular Python libraries like scikit learn for our machine learning applications. Which means we need to use PySpark's MLlib packages for these tasks. Luckily, MLlib offers a pretty good variety of algorithms! In this notebook we will go over how to prep our data and train and test the classification algorithms PySpark offers. \n",
    "\n",
    "## Algorithms Available\n",
    "\n",
    "PySpark offers the following algorithms for classification. \n",
    "\n",
    "1. Logistic Regression \n",
    "2. Naive Bayes\n",
    "3. One Vs Rest\n",
    "4. Linear Support Vector Machine (SVC)\n",
    "5. Random Forest Classifier\n",
    "6. GBT Classifier\n",
    "7. Decision Tree Classifier\n",
    "8. Multilayer Perceptron Classifier (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "vanilla-forward",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-2NP7FHU.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>classification</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2053aa1a340>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing pysark and creating a session\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('classification').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "powerful-hungarian",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "closed-database",
   "metadata": {},
   "source": [
    "### Data Set Name: Autistic Spectrum Disorder Screening Data for Adult\n",
    "Autistic Spectrum Disorder (ASD) is a neurodevelopment condition associated with significant healthcare costs, and early diagnosis can significantly reduce these. Unfortunately, waiting times for an ASD diagnosis are lengthy and procedures are not cost effective. The economic impact of autism and the increase in the number of ASD cases across the world reveals an urgent need for the development of easily implemented and effective screening methods. Therefore, a time-efficient and accessible ASD screening is imminent to help health professionals and inform individuals whether they should pursue formal clinical diagnosis. The rapid growth in the number of ASD cases worldwide necessitates datasets related to behaviour traits. However, such datasets are rare making it difficult to perform thorough analyses to improve the efficiency, sensitivity, specificity and predictive accuracy of the ASD screening process. Presently, very limited autism datasets associated with clinical or screening are available and most of them are genetic in nature. Hence, we propose a new dataset related to autism screening of adults that contained 20 features to be utilised for further analysis especially in determining influential autistic traits and improving the classification of ASD cases. In this dataset, we record ten behavioural features (AQ-10-Adult) plus ten individuals characteristics that have proved to be effective in detecting the ASD cases from controls in behaviour science.\n",
    "\n",
    "### Source: \n",
    "https://www.kaggle.com/faizunnabi/autism-screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "normal-happiness",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"datasets-mlib/\"\n",
    "df = spark.read.csv(path+'Toddler Autism dataset July 2018.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "powerful-tuner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case_No</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>Age_Mons</th>\n",
       "      <th>Qchat-10-Score</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Jaundice</th>\n",
       "      <th>Family_mem_with_ASD</th>\n",
       "      <th>Who completed the test</th>\n",
       "      <th>Class/ASD Traits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>f</td>\n",
       "      <td>middle eastern</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>m</td>\n",
       "      <td>White European</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>m</td>\n",
       "      <td>middle eastern</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>m</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>f</td>\n",
       "      <td>White European</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>m</td>\n",
       "      <td>black</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case_No  A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  Age_Mons  Qchat-10-Score  \\\n",
       "0        1   0   0   0   0   0   0   1   1   0    1        28               3   \n",
       "1        2   1   1   0   0   0   1   1   0   0    0        36               4   \n",
       "2        3   1   0   0   0   0   0   1   1   0    1        36               4   \n",
       "3        4   1   1   1   1   1   1   1   1   1    1        24              10   \n",
       "4        5   1   1   0   1   1   1   1   1   1    1        20               9   \n",
       "5        6   1   1   0   0   1   1   1   1   1    1        21               8   \n",
       "\n",
       "  Sex       Ethnicity Jaundice Family_mem_with_ASD Who completed the test  \\\n",
       "0   f  middle eastern      yes                  no          family member   \n",
       "1   m  White European      yes                  no          family member   \n",
       "2   m  middle eastern      yes                  no          family member   \n",
       "3   m        Hispanic       no                  no          family member   \n",
       "4   f  White European       no                 yes          family member   \n",
       "5   m           black       no                  no          family member   \n",
       "\n",
       "  Class/ASD Traits   \n",
       "0                No  \n",
       "1               Yes  \n",
       "2               Yes  \n",
       "3               Yes  \n",
       "4               Yes  \n",
       "5               Yes  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the dataset\n",
    "df.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "scientific-release",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Case_No: integer (nullable = true)\n",
      " |-- A1: integer (nullable = true)\n",
      " |-- A2: integer (nullable = true)\n",
      " |-- A3: integer (nullable = true)\n",
      " |-- A4: integer (nullable = true)\n",
      " |-- A5: integer (nullable = true)\n",
      " |-- A6: integer (nullable = true)\n",
      " |-- A7: integer (nullable = true)\n",
      " |-- A8: integer (nullable = true)\n",
      " |-- A9: integer (nullable = true)\n",
      " |-- A10: integer (nullable = true)\n",
      " |-- Age_Mons: integer (nullable = true)\n",
      " |-- Qchat-10-Score: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Ethnicity: string (nullable = true)\n",
      " |-- Jaundice: string (nullable = true)\n",
      " |-- Family_mem_with_ASD: string (nullable = true)\n",
      " |-- Who completed the test: string (nullable = true)\n",
      " |-- Class/ASD Traits : string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-commander",
   "metadata": {},
   "source": [
    "In the dataset,\n",
    "- Inependent variables (features): Case_No - Who completed the test\n",
    "- Dependent variable: Class/ASD Traits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "classified-london",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Class/ASD Traits |count|\n",
      "+-----------------+-----+\n",
      "|               No|  326|\n",
      "|              Yes|  728|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identifying the number of classes in the dpenedent variable\n",
    "df.groupBy(\"Class/ASD Traits \").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-correspondence",
   "metadata": {},
   "source": [
    "### Formatting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "induced-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_columns = df.columns # Collect the column names as a list\n",
    "input_columns = input_columns[1:-1] # keep only relevant columns: from column 1 to \n",
    "\n",
    "dependent_var = 'Class/ASD Traits '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "disturbed-elite",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Changing the label (class variable) to string type to prep for reindexing\n",
    "# Pyspark expects a zero indexed integer for the label column\n",
    "# Just in case our data is not in that format... we will treat it by using the StringIndexer built in method\n",
    "renamed = df.withColumn(\"label_str\", df[dependent_var].cast(StringType())) #Rename and change to string type\n",
    "indexer = StringIndexer(inputCol=\"label_str\", outputCol=\"label\") #Pyspark is expecting the this naming convention \n",
    "indexed = indexer.fit(renamed).transform(renamed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "filled-duplicate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all string type data in the input column list to numeric, otherwise the Algorithm will not be able to process it\n",
    "# Also we will use these lists later on\n",
    "numeric_inputs = []\n",
    "string_inputs = []\n",
    "for column in input_columns:\n",
    "    # First identify the string vars in your input column list\n",
    "    if str(indexed.schema[column].dataType) == 'StringType':\n",
    "        # Setting up the String Indexer function\n",
    "        indexer = StringIndexer(inputCol=column, outputCol=column+\"_num\") \n",
    "        # Then call on the indexer created here\n",
    "        indexed = indexer.fit(indexed).transform(indexed)\n",
    "        # Rename the column to a new name so you can disinguish it from the original\n",
    "        new_col_name = column+\"_num\"\n",
    "        # Adding the new column name to the string inputs list\n",
    "        string_inputs.append(new_col_name)\n",
    "    else:\n",
    "        # If no change was needed, take no action \n",
    "        # And add the numeric var to the num list\n",
    "        numeric_inputs.append(column)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assumed-death",
   "metadata": {},
   "source": [
    "#### Treating for skewness and outliers\n",
    "\n",
    "Skewness measures how much a distribution of values deviates from symmetry around the mean. A value of zero means the distribution is symmetric, while a positive skewness indicates a greater number of smaller values, and a negative value indicates a greater number of larger values. \n",
    "\n",
    "As a general rule of thumb: \n",
    "\n",
    " - If skewness is **less than -1 or greater than 1**, the distribution is highly skewed. \n",
    " - If skewness is **between -1 and -0.5 or between 0.5 and 1**, the distribution is moderately skewed. \n",
    " - If skewness is **between -0.5 and 0.5**, the distribution is approximately symmetric.\n",
    " \n",
    "A common recommendation for treating skewness is either a log transformation for positive skewed data or an exponential transformation for negatively skewed data.\n",
    "\n",
    "\n",
    "**Outliers** <br>\n",
    "One common way to correct outliers is by flooring and capping which means editing any value that is above or below a certain threshold (99th percentile or 1st percentile) back to the highest/lowest value in that percentile. For example, if the 99th percentile is 96 and there is a value of 1,000, you would change that value to 96. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "supported-deposit",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty dictionary d\n",
    "d = {}\n",
    "# Creating a dictionary of quantiles from numeric cols, Doing the top and bottom 1% but it can be adjusted if needed\n",
    "for col in numeric_inputs: \n",
    "    d[col] = indexed.approxQuantile(col,[0.01,0.99],0.25) #if you want to make it go faster increase the last number\n",
    "\n",
    "#Now check for skewness for all numeric cols\n",
    "for col in numeric_inputs:\n",
    "    skew = indexed.agg(skewness(indexed[col])).collect() #check for skewness\n",
    "    skew = skew[0][0]\n",
    "    # If skewness is found,\n",
    "    # This function will make the appropriate corrections\n",
    "    if skew > 1: # If right skew, floor, cap and log(x+1)\n",
    "        indexed = indexed.withColumn(col, \\\n",
    "        log(when(df[col] < d[col][0],d[col][0])\\\n",
    "        .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "        .otherwise(indexed[col] ) +1).alias(col))\n",
    "        print(col+\" has been treated for positive (right) skewness. (skew =)\",skew,\")\")\n",
    "    elif skew < -1: # If left skew floor, cap and exp(x)\n",
    "        indexed = indexed.withColumn(col, \\\n",
    "        exp(when(df[col] < d[col][0],d[col][0])\\\n",
    "        .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "        .otherwise(indexed[col] )).alias(col))\n",
    "        print(col+\" has been treated for negative (left) skewness. (skew =\",skew,\")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "silver-houston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No negative values were found in your dataframe.\n"
     ]
    }
   ],
   "source": [
    "# Checking for negative values in the dataframe \n",
    "# Produce a warning if there are negative values in the dataframe that Naive Bayes cannot be used. \n",
    "# Note: Checking only the numeric input values since anything that is indexed won't have negative values\n",
    "\n",
    "# Calculate the mins for all columns in the df\n",
    "minimums = df.select([min(c).alias(c) for c in df.columns if c in numeric_inputs]) \n",
    "# Create an array for all mins and select only the input cols\n",
    "min_array = minimums.select(array(numeric_inputs).alias(\"mins\")) \n",
    "# Collect golobal min as Python object\n",
    "df_minimum = min_array.select(array_min(min_array.mins)).collect() \n",
    "# Slice to get the number itself\n",
    "df_minimum = df_minimum[0][0] \n",
    "\n",
    "# If there are ANY negative vals found in the df, print a warning message\n",
    "if df_minimum < 0:\n",
    "    print(\"WARNING: The Naive Bayes Classifier will not be able to process your dataframe as it contains negative values\")\n",
    "else:\n",
    "    print(\"No negative values were found in your dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "private-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before correcting for negative values that may have been found above, there is a need to vectorize our df\n",
    "# because the function that I'm using to make that correction requires a vector\n",
    "# Now creating final features list\n",
    "features_list = numeric_inputs + string_inputs\n",
    "# Creating vector assembler object\n",
    "assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
    "# And calling on the vector assembler to transform your dataframe\n",
    "output = assembler.transform(indexed).select('features','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "innocent-thickness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled to range: [0.000000, 1000.000000]\n",
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|(17,[6,7,9,10,11,...|\n",
      "|  0.0|(17,[0,1,5,6,10,1...|\n",
      "|  0.0|(17,[0,6,7,9,10,1...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  0.0|[1000.0,1000.0,0....|\n",
      "|  0.0|[1000.0,1000.0,0....|\n",
      "|  0.0|(17,[0,3,4,5,8,10...|\n",
      "|  0.0|(17,[1,4,6,7,8,9,...|\n",
      "|  1.0|(17,[6,9,10,11,13...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  0.0|[1000.0,0.0,0.0,1...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  1.0|(17,[10,12,13,14]...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  1.0|(17,[10,13],[250....|\n",
      "|  0.0|(17,[0,1,2,4,6,7,...|\n",
      "|  1.0|(17,[10,13,15],[1...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  1.0|(17,[0,4,9,10,11,...|\n",
      "|  0.0|(17,[0,1,2,4,6,7,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Creating the mix max scaler object, this is what will correct for negative values\n",
    "# I like to use a high range like 1,000 because I only see one decimal place in the final_data.show() call\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",min=0,max=1000)\n",
    "print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "\n",
    "# Computing summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(output)\n",
    "\n",
    "# Rescaling each feature to range [min, max].\n",
    "scaled_data = scalerModel.transform(output)\n",
    "final_data = scaled_data.select('label','scaledFeatures')\n",
    "# Renaming to default value\n",
    "final_data = final_data.withColumnRenamed(\"scaledFeatures\",\"features\")\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "sunset-victim",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into train and test\n",
    "train, test = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "severe-duration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "752"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "executive-plumbing",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "302"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "physical-victory",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "suffering-blocking",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "pointed-capitol",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up our evaluation objects\n",
    "Bin_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction')\n",
    "MC_evaluator = MulticlassClassificationEvaluator(metricName='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "deluxe-worse",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiatng logistic regression constructor\n",
    "classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "polished-welcome",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitModel = classifier.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bottom-breakdown",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation method for multiclass classification problem\n",
    "predictions = fitModel.transform(test)\n",
    "accuracy = (MC_evaluator.evaluate(predictions))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "hourly-builder",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 100.0\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cleared-beauty",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
