{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "confused-buyer",
   "metadata": {},
   "source": [
    "# Classification in PySpark's MLlib\n",
    "\n",
    "PySpark offers a good variety of algorithms that can be applied to classification machine learning problems. However, because PySpark operates on distributed dataframes, we cannot use popular Python libraries like scikit learn for our machine learning applications. Which means we need to use PySpark's MLlib packages for these tasks. Luckily, MLlib offers a pretty good variety of algorithms! In this notebook we will go over how to prep our data and train and test the classification algorithms PySpark offers. \n",
    "\n",
    "## Algorithms Available\n",
    "\n",
    "PySpark offers the following algorithms for classification. \n",
    "\n",
    "1. Logistic Regression \n",
    "2. Naive Bayes\n",
    "3. One Vs Rest\n",
    "4. Linear Support Vector Machine (SVC)\n",
    "5. Random Forest Classifier\n",
    "6. GBT Classifier\n",
    "7. Decision Tree Classifier\n",
    "8. Multilayer Perceptron Classifier (Neural Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dedicated-folks",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://DESKTOP-2NP7FHU.mshome.net:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>classification1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x2503e9ba340>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importing pysark and creating a session\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('classification1').getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "governmental-architect",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-value",
   "metadata": {},
   "source": [
    "### Data Set Name: Autistic Spectrum Disorder Screening Data for Adult\n",
    "Autistic Spectrum Disorder (ASD) is a neurodevelopment condition associated with significant healthcare costs, and early diagnosis can significantly reduce these. Unfortunately, waiting times for an ASD diagnosis are lengthy and procedures are not cost effective. The economic impact of autism and the increase in the number of ASD cases across the world reveals an urgent need for the development of easily implemented and effective screening methods. Therefore, a time-efficient and accessible ASD screening is imminent to help health professionals and inform individuals whether they should pursue formal clinical diagnosis. The rapid growth in the number of ASD cases worldwide necessitates datasets related to behaviour traits. However, such datasets are rare making it difficult to perform thorough analyses to improve the efficiency, sensitivity, specificity and predictive accuracy of the ASD screening process. Presently, very limited autism datasets associated with clinical or screening are available and most of them are genetic in nature. Hence, we propose a new dataset related to autism screening of adults that contained 20 features to be utilised for further analysis especially in determining influential autistic traits and improving the classification of ASD cases. In this dataset, we record ten behavioural features (AQ-10-Adult) plus ten individuals characteristics that have proved to be effective in detecting the ASD cases from controls in behaviour science.\n",
    "\n",
    "### Source: \n",
    "https://www.kaggle.com/faizunnabi/autism-screening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "empty-softball",
   "metadata": {},
   "outputs": [],
   "source": [
    "path =\"datasets-mlib/\"\n",
    "df = spark.read.csv(path+'Toddler Autism dataset July 2018.csv',inferSchema=True,header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eastern-minority",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case_No</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>A10</th>\n",
       "      <th>Age_Mons</th>\n",
       "      <th>Qchat-10-Score</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Jaundice</th>\n",
       "      <th>Family_mem_with_ASD</th>\n",
       "      <th>Who completed the test</th>\n",
       "      <th>Class/ASD Traits</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>f</td>\n",
       "      <td>middle eastern</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>m</td>\n",
       "      <td>White European</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>m</td>\n",
       "      <td>middle eastern</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>m</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>f</td>\n",
       "      <td>White European</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>8</td>\n",
       "      <td>m</td>\n",
       "      <td>black</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case_No  A1  A2  A3  A4  A5  A6  A7  A8  A9  A10  Age_Mons  Qchat-10-Score  \\\n",
       "0        1   0   0   0   0   0   0   1   1   0    1        28               3   \n",
       "1        2   1   1   0   0   0   1   1   0   0    0        36               4   \n",
       "2        3   1   0   0   0   0   0   1   1   0    1        36               4   \n",
       "3        4   1   1   1   1   1   1   1   1   1    1        24              10   \n",
       "4        5   1   1   0   1   1   1   1   1   1    1        20               9   \n",
       "5        6   1   1   0   0   1   1   1   1   1    1        21               8   \n",
       "\n",
       "  Sex       Ethnicity Jaundice Family_mem_with_ASD Who completed the test  \\\n",
       "0   f  middle eastern      yes                  no          family member   \n",
       "1   m  White European      yes                  no          family member   \n",
       "2   m  middle eastern      yes                  no          family member   \n",
       "3   m        Hispanic       no                  no          family member   \n",
       "4   f  White European       no                 yes          family member   \n",
       "5   m           black       no                  no          family member   \n",
       "\n",
       "  Class/ASD Traits   \n",
       "0                No  \n",
       "1               Yes  \n",
       "2               Yes  \n",
       "3               Yes  \n",
       "4               Yes  \n",
       "5               Yes  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the dataset\n",
    "df.limit(6).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "yellow-cliff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Case_No: integer (nullable = true)\n",
      " |-- A1: integer (nullable = true)\n",
      " |-- A2: integer (nullable = true)\n",
      " |-- A3: integer (nullable = true)\n",
      " |-- A4: integer (nullable = true)\n",
      " |-- A5: integer (nullable = true)\n",
      " |-- A6: integer (nullable = true)\n",
      " |-- A7: integer (nullable = true)\n",
      " |-- A8: integer (nullable = true)\n",
      " |-- A9: integer (nullable = true)\n",
      " |-- A10: integer (nullable = true)\n",
      " |-- Age_Mons: integer (nullable = true)\n",
      " |-- Qchat-10-Score: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Ethnicity: string (nullable = true)\n",
      " |-- Jaundice: string (nullable = true)\n",
      " |-- Family_mem_with_ASD: string (nullable = true)\n",
      " |-- Who completed the test: string (nullable = true)\n",
      " |-- Class/ASD Traits : string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-tribute",
   "metadata": {},
   "source": [
    "In the dataset,\n",
    "- Inependent variables (features): Case_No - Who completed the test\n",
    "- Dependent variable: Class/ASD Traits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dental-charm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----+\n",
      "|Class/ASD Traits |count|\n",
      "+-----------------+-----+\n",
      "|               No|  326|\n",
      "|              Yes|  728|\n",
      "+-----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Identifying the number of classes in the dpenedent variable\n",
    "df.groupBy(\"Class/ASD Traits \").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-gnome",
   "metadata": {},
   "source": [
    "### Formatting data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hairy-moore",
   "metadata": {},
   "source": [
    "MLib requires all the data to be vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "broken-payment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A1',\n",
       " 'A2',\n",
       " 'A3',\n",
       " 'A4',\n",
       " 'A5',\n",
       " 'A6',\n",
       " 'A7',\n",
       " 'A8',\n",
       " 'A9',\n",
       " 'A10',\n",
       " 'Age_Mons',\n",
       " 'Qchat-10-Score',\n",
       " 'Sex',\n",
       " 'Ethnicity',\n",
       " 'Jaundice',\n",
       " 'Family_mem_with_ASD',\n",
       " 'Who completed the test']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Separating dependent and independent variables\n",
    "input_columns = df.columns # Collect the column names as a list\n",
    "input_columns = input_columns[1:-1] # keep only relevant columns: from column 1 to \n",
    "input_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "choice-relation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Class/ASD Traits '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dependent_var = 'Class/ASD Traits '\n",
    "dependent_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "sudden-proposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convwrting the dependent variables columsn from yes, n to 0 and 1\n",
    "renamed = df.withColumn(\"label_str\", df[dependent_var].cast(StringType())) #Rename and change to string type\n",
    "indexer = StringIndexer(inputCol=\"label_str\", outputCol=\"label\") #Pyspark is expecting the this naming convention "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "thousand-voluntary",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case_No</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>...</th>\n",
       "      <th>Age_Mons</th>\n",
       "      <th>Qchat-10-Score</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Ethnicity</th>\n",
       "      <th>Jaundice</th>\n",
       "      <th>Family_mem_with_ASD</th>\n",
       "      <th>Who completed the test</th>\n",
       "      <th>Class/ASD Traits</th>\n",
       "      <th>label_str</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>28</td>\n",
       "      <td>3</td>\n",
       "      <td>f</td>\n",
       "      <td>middle eastern</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>m</td>\n",
       "      <td>White European</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>36</td>\n",
       "      <td>4</td>\n",
       "      <td>m</td>\n",
       "      <td>middle eastern</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>10</td>\n",
       "      <td>m</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>20</td>\n",
       "      <td>9</td>\n",
       "      <td>f</td>\n",
       "      <td>White European</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case_No  A1  A2  A3  A4  A5  A6  A7  A8  A9  ...  Age_Mons  Qchat-10-Score  \\\n",
       "0        1   0   0   0   0   0   0   1   1   0  ...        28               3   \n",
       "1        2   1   1   0   0   0   1   1   0   0  ...        36               4   \n",
       "2        3   1   0   0   0   0   0   1   1   0  ...        36               4   \n",
       "3        4   1   1   1   1   1   1   1   1   1  ...        24              10   \n",
       "4        5   1   1   0   1   1   1   1   1   1  ...        20               9   \n",
       "\n",
       "   Sex       Ethnicity Jaundice Family_mem_with_ASD Who completed the test  \\\n",
       "0    f  middle eastern      yes                  no          family member   \n",
       "1    m  White European      yes                  no          family member   \n",
       "2    m  middle eastern      yes                  no          family member   \n",
       "3    m        Hispanic       no                  no          family member   \n",
       "4    f  White European       no                 yes          family member   \n",
       "\n",
       "  Class/ASD Traits  label_str label  \n",
       "0                No        No   1.0  \n",
       "1               Yes       Yes   0.0  \n",
       "2               Yes       Yes   0.0  \n",
       "3               Yes       Yes   0.0  \n",
       "4               Yes       Yes   0.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed = indexer.fit(renamed).transform(renamed)\n",
    "indexed.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "happy-distributor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converting all the string type data in the input column list to numeric, otherwise pyspark cannot process it\n",
    "numeric_inputs = []\n",
    "string_inputs = []\n",
    "for column in input_columns:\n",
    "    #Identifying string type variables\n",
    "    if str(indexed.schema[column].dataType) == 'StringType':\n",
    "        #Setting up string indexer function \n",
    "        indexer = StringIndexer(inputCol=column, outputCol=column+\"_num\")\n",
    "        #Calling the indexer function\n",
    "        indexed = indexer.fit(indexed).transform(indexed)\n",
    "        #Collecting the new column names\n",
    "        new_col_name = column+\"_num\"\n",
    "        #Appending the column name to string input list\n",
    "        string_inputs.append(new_col_name)\n",
    "    else:\n",
    "        numeric_inputs.append(column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "systematic-entrance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Case_No</th>\n",
       "      <th>A1</th>\n",
       "      <th>A2</th>\n",
       "      <th>A3</th>\n",
       "      <th>A4</th>\n",
       "      <th>A5</th>\n",
       "      <th>A6</th>\n",
       "      <th>A7</th>\n",
       "      <th>A8</th>\n",
       "      <th>A9</th>\n",
       "      <th>...</th>\n",
       "      <th>Family_mem_with_ASD</th>\n",
       "      <th>Who completed the test</th>\n",
       "      <th>Class/ASD Traits</th>\n",
       "      <th>label_str</th>\n",
       "      <th>label</th>\n",
       "      <th>Sex_num</th>\n",
       "      <th>Ethnicity_num</th>\n",
       "      <th>Jaundice_num</th>\n",
       "      <th>Family_mem_with_ASD_num</th>\n",
       "      <th>Who completed the test_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>no</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>yes</td>\n",
       "      <td>family member</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Case_No  A1  A2  A3  A4  A5  A6  A7  A8  A9  ...  Family_mem_with_ASD  \\\n",
       "0        1   0   0   0   0   0   0   1   1   0  ...                   no   \n",
       "1        2   1   1   0   0   0   1   1   0   0  ...                   no   \n",
       "2        3   1   0   0   0   0   0   1   1   0  ...                   no   \n",
       "3        4   1   1   1   1   1   1   1   1   1  ...                   no   \n",
       "4        5   1   1   0   1   1   1   1   1   1  ...                  yes   \n",
       "\n",
       "   Who completed the test  Class/ASD Traits  label_str label Sex_num  \\\n",
       "0           family member                 No        No   1.0     1.0   \n",
       "1           family member                Yes       Yes   0.0     0.0   \n",
       "2           family member                Yes       Yes   0.0     0.0   \n",
       "3           family member                Yes       Yes   0.0     0.0   \n",
       "4           family member                Yes       Yes   0.0     1.0   \n",
       "\n",
       "  Ethnicity_num Jaundice_num Family_mem_with_ASD_num  \\\n",
       "0           2.0          1.0                     0.0   \n",
       "1           0.0          1.0                     0.0   \n",
       "2           2.0          1.0                     0.0   \n",
       "3           5.0          0.0                     0.0   \n",
       "4           0.0          0.0                     1.0   \n",
       "\n",
       "  Who completed the test_num  \n",
       "0                        0.0  \n",
       "1                        0.0  \n",
       "2                        0.0  \n",
       "3                        0.0  \n",
       "4                        0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed.limit(5).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "valid-graham",
   "metadata": {},
   "source": [
    "#### Treating for skewness and outliers\n",
    "\n",
    "Skewness measures how much a distribution of values deviates from symmetry around the mean. A value of zero means the distribution is symmetric, while a positive skewness indicates a greater number of smaller values, and a negative value indicates a greater number of larger values. \n",
    "\n",
    "As a general rule of thumb: \n",
    "\n",
    " - If skewness is **less than -1 or greater than 1**, the distribution is highly skewed. \n",
    " - If skewness is **between -1 and -0.5 or between 0.5 and 1**, the distribution is moderately skewed. \n",
    " - If skewness is **between -0.5 and 0.5**, the distribution is approximately symmetric.\n",
    " \n",
    "A common recommendation for treating skewness is either a log transformation for positive skewed data or an exponential transformation for negatively skewed data.\n",
    "\n",
    "\n",
    "**Outliers** <br>\n",
    "One common way to correct outliers is by flooring and capping which means editing any value that is above or below a certain threshold (99th percentile or 1st percentile) back to the highest/lowest value in that percentile. For example, if the 99th percentile is 96 and there is a value of 1,000, you would change that value to 96. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "elegant-bailey",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating empty dictionary d\n",
    "d = {}\n",
    "# Creating a dictionary of quantiles from numeric cols, Doing the top and bottom 1% but it can be adjusted if needed\n",
    "for col in numeric_inputs: \n",
    "    d[col] = indexed.approxQuantile(col,[0.01,0.99],0.25) #if you want to make it go faster increase the last number\n",
    "\n",
    "#Now check for skewness for all numeric cols\n",
    "for col in numeric_inputs:\n",
    "    skew = indexed.agg(skewness(indexed[col])).collect() #check for skewness\n",
    "    skew = skew[0][0]\n",
    "    # If skewness is found,\n",
    "    # This function will make the appropriate corrections\n",
    "    if skew > 1: # If right skew, floor, cap and log(x+1)\n",
    "        indexed = indexed.withColumn(col, \\\n",
    "        log(when(df[col] < d[col][0],d[col][0])\\\n",
    "        .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "        .otherwise(indexed[col] ) +1).alias(col))\n",
    "        print(col+\" has been treated for positive (right) skewness. (skew =)\",skew,\")\")\n",
    "    elif skew < -1: # If left skew floor, cap and exp(x)\n",
    "        indexed = indexed.withColumn(col, \\\n",
    "        exp(when(df[col] < d[col][0],d[col][0])\\\n",
    "        .when(indexed[col] > d[col][1], d[col][1])\\\n",
    "        .otherwise(indexed[col] )).alias(col))\n",
    "        print(col+\" has been treated for negative (left) skewness. (skew =\",skew,\")\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dutch-flood",
   "metadata": {},
   "source": [
    "As I didn't get any print statements executed above, I can conclude the data is not treated for skewness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "studied-prevention",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No negative values were found in your dataframe.\n"
     ]
    }
   ],
   "source": [
    "#Checking for negative values in the dataframe \n",
    "#Producign a warning if there are negative values in the dataframe that Naive Bayes cannot be used\n",
    "#Note: Checking only the numeric input values since anything that is indexed won't have negative values\n",
    "\n",
    "# Calculating the mins for all columns in the df\n",
    "minimums = df.select([min(c).alias(c) for c in df.columns if c in numeric_inputs]) \n",
    "# Creating an array for all mins and select only the input cols\n",
    "min_array = minimums.select(array(numeric_inputs).alias(\"mins\")) \n",
    "# Collecting global min as Python object\n",
    "df_minimum = min_array.select(array_min(min_array.mins)).collect() \n",
    "# Slicing to get the number itself\n",
    "df_minimum = df_minimum[0][0] \n",
    "\n",
    "# If there are ANY negative vals found in the df, print a warning message\n",
    "if df_minimum < 0:\n",
    "    print(\"WARNING: The Naive Bayes Classifier will not be able to process your dataframe as it contains negative values\")\n",
    "else:\n",
    "    print(\"No negative values were found in your dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "persistent-pakistan",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now creating final features list\n",
    "features_list = numeric_inputs + string_inputs\n",
    "# Creating vector assembler object\n",
    "assembler = VectorAssembler(inputCols=features_list,outputCol='features')\n",
    "# And calling on the vector assembler to transform your dataframe\n",
    "output = assembler.transform(indexed).select('features','label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ideal-ghost",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(17,[6,7,9,10,11,...|  1.0|\n",
      "|(17,[0,1,5,6,10,1...|  0.0|\n",
      "|(17,[0,6,7,9,10,1...|  0.0|\n",
      "|[1.0,1.0,1.0,1.0,...|  0.0|\n",
      "|[1.0,1.0,0.0,1.0,...|  0.0|\n",
      "|[1.0,1.0,0.0,0.0,...|  0.0|\n",
      "|(17,[0,3,4,5,8,10...|  0.0|\n",
      "|(17,[1,4,6,7,8,9,...|  0.0|\n",
      "|(17,[6,9,10,11,13...|  1.0|\n",
      "|[1.0,1.0,1.0,0.0,...|  0.0|\n",
      "|[1.0,0.0,0.0,1.0,...|  0.0|\n",
      "|[1.0,1.0,1.0,1.0,...|  0.0|\n",
      "|(17,[10,12,13,14]...|  1.0|\n",
      "|[1.0,1.0,1.0,1.0,...|  0.0|\n",
      "|(17,[10,13],[18.0...|  1.0|\n",
      "|(17,[0,1,2,4,6,7,...|  0.0|\n",
      "|(17,[10,13,15],[3...|  1.0|\n",
      "|[1.0,1.0,1.0,0.0,...|  0.0|\n",
      "|(17,[0,4,9,10,11,...|  1.0|\n",
      "|[1.0,1.0,1.0,0.0,...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "existing-edition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled to range: [0.000000, 1000.000000]\n"
     ]
    }
   ],
   "source": [
    "# Creating the mix max scaler object (process if there are negative values present)\n",
    "#Udsing range 0 to 1000\n",
    "\n",
    "scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\",min=0,max=1000)\n",
    "print(\"Features scaled to range: [%f, %f]\" % (scaler.getMin(), scaler.getMax()))\n",
    "# Computing summary statistics and generate MinMaxScalerModel\n",
    "scalerModel = scaler.fit(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "athletic-ownership",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[features: vector, label: double, scaledFeatures: vector]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Rescaling each feature to range [min, max].\n",
    "scaled_data = scalerModel.transform(output)\n",
    "scaled_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "accompanied-constitution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  1.0|(17,[6,7,9,10,11,...|\n",
      "|  0.0|(17,[0,1,5,6,10,1...|\n",
      "|  0.0|(17,[0,6,7,9,10,1...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  0.0|[1000.0,1000.0,0....|\n",
      "|  0.0|[1000.0,1000.0,0....|\n",
      "|  0.0|(17,[0,3,4,5,8,10...|\n",
      "|  0.0|(17,[1,4,6,7,8,9,...|\n",
      "|  1.0|(17,[6,9,10,11,13...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  0.0|[1000.0,0.0,0.0,1...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  1.0|(17,[10,12,13,14]...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  1.0|(17,[10,13],[250....|\n",
      "|  0.0|(17,[0,1,2,4,6,7,...|\n",
      "|  1.0|(17,[10,13,15],[1...|\n",
      "|  0.0|[1000.0,1000.0,10...|\n",
      "|  1.0|(17,[0,4,9,10,11,...|\n",
      "|  0.0|(17,[0,1,2,4,6,7,...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data = scaled_data.select('label','scaledFeatures')\n",
    "# Renaming to default value\n",
    "final_data = final_data.withColumnRenamed(\"scaledFeatures\",\"features\")\n",
    "final_data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-finland",
   "metadata": {},
   "source": [
    "#### Splitting the data into training and splitting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "suffering-buffer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting the data into train and test\n",
    "train, test = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "devoted-incentive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "759"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "civilian-toyota",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "295"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "worldwide-prisoner",
   "metadata": {},
   "source": [
    "##### Reading in dependencies from PySpark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "herbal-niagara",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import *\n",
    "from pyspark.ml.evaluation import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "beginning-mexican",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up our evaluation objects\n",
    "binary_eval = BinaryClassificationEvaluator(rawPredictionCol='prediction')\n",
    "MC_eval = MulticlassClassificationEvaluator(metricName='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-angle",
   "metadata": {},
   "source": [
    "### Implementing Logistic Regression\n",
    "The Logistic Regression Algorithm, also known as \"Logit\", is used to estimate (guess) the probability (a number between 0 and 1) of an event occurring having been given some previous data to “learn” from. It works with either binary or multinomial (more than 2 categories) data and uses logistic function (ie. log) to find a model that fits with the data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "together-establishment",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initiatng logistic regression constructor\n",
    "classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "decimal-effort",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitModel = classifier.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "angry-advertising",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Evaluation method for binary classification problem\n",
    "predictionAndLabels = fitModel.transform(test)\n",
    "auc = binary_eval.evaluate(predictionAndLabels)\n",
    "print(\"AUC:\",auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amber-crossing",
   "metadata": {},
   "source": [
    "#### Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ideal-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the classifier\n",
    "classifier = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "existing-journey",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Setting up parameter grid for the cross validator\n",
    "paramGrid = (ParamGridBuilder().addGrid(classifier.maxIter, [10,15,20]).build())\n",
    "\n",
    "#Setting up cross validator\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                         estimatorParamMaps=paramGrid,\n",
    "                         evaluator=MC_eval,\n",
    "                         numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "occupied-patent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "fitModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "responsible-professor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept:[51.71794521838649]\n",
      "CoefficientsDenseMatrix([[-0.01162562, -0.01066733, -0.01043395, -0.0117568 , -0.010962  ,\n",
      "              -0.01144383, -0.01162115, -0.01056535, -0.01129973, -0.01105928,\n",
      "              -0.00050193, -0.03290132, -0.00015347, -0.00196149, -0.00026341,\n",
      "               0.00017106, -0.00143897]])\n"
     ]
    }
   ],
   "source": [
    "#Collecting the best model\n",
    "BestModel = fitModel.bestModel\n",
    "print(\"Intercept:\" + str(BestModel.interceptVector))\n",
    "print(\"Coefficients\" + str(BestModel.coefficientMatrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "approximate-fashion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best model: LogisticRegressionModel: uid=LogisticRegression_9fbcd87d07a4, numClasses=2, numFeatures=17\n",
      "Accuracy: 100.0\n"
     ]
    }
   ],
   "source": [
    "#We can extract the best model from this run like below\n",
    "LR_BestModel = BestModel\n",
    "print(\"Best model:\", LR_BestModel)\n",
    "#Generating the prediction\n",
    "predictions = fitModel.transform(test)\n",
    "\n",
    "#Calculating the accuracy\n",
    "accuracy = (MC_eval.evaluate(predictions))*100\n",
    "print(\"Accuracy:\",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inappropriate-offense",
   "metadata": {},
   "source": [
    "#### Classification Diagnostics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "hawaiian-keyboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the summary\n",
    "trainingSummary = LR_BestModel.summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "interracial-anchor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+\n",
      "|summary|             label|        prediction|\n",
      "+-------+------------------+------------------+\n",
      "|  count|               759|               759|\n",
      "|   mean| 0.310935441370224| 0.310935441370224|\n",
      "| stddev|0.4631816603062981|0.4631816603062981|\n",
      "|    min|               0.0|               0.0|\n",
      "|    max|               1.0|               1.0|\n",
      "+-------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate descibe\n",
    "trainingSummary.predictions.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "national-smart",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Objective History at each iteration\n",
      "0.6198470847843046\n",
      "0.51854176071745\n",
      "0.45458359771239726\n",
      "0.286090535418719\n",
      "0.2581878313998878\n",
      "0.22019114416073587\n",
      "0.19985339704802196\n",
      "0.1777687572970849\n",
      "0.14257636964308962\n",
      "0.08415978728826765\n",
      "0.05660084996068759\n",
      "0.02703699402904461\n",
      "0.017672459276642388\n",
      "0.011628025824695512\n",
      "0.006500667503663923\n",
      "0.005577037685455732\n",
      "0.002711847204264477\n",
      "0.0015241918832879177\n",
      "0.0007732832507238134\n",
      "0.00039497815976479193\n",
      "0.0001977556235196983\n"
     ]
    }
   ],
   "source": [
    "#Obtaining the objective per iteration\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print(\"Objective History at each iteration\")\n",
    "for objective in objectiveHistory:\n",
    "    print(objective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "binary-pride",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "False positive rate by label:\n",
      "label 0: 0.0\n",
      "label 1: 0.0\n",
      " \n",
      "True positive rate by label:\n",
      "label 0: 1.0\n",
      "label 1: 1.0\n",
      " \n",
      "Precision by label:\n",
      "label 0: 1.0\n",
      "label 1: 1.0\n",
      " \n",
      "Recall by label:\n",
      "label 0: 1.0\n",
      "label 1: 1.0\n",
      " \n",
      "F-measure by label:\n",
      "label 0: 1.0\n",
      "label 1: 1.0\n"
     ]
    }
   ],
   "source": [
    "#For multiclass, we can inspect metrics on a per-label basis\n",
    "print(\" \")\n",
    "print(\"False positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.falsePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\" \")\n",
    "print(\"True positive rate by label:\")\n",
    "for i, rate in enumerate(trainingSummary.truePositiveRateByLabel):\n",
    "    print(\"label %d: %s\" % (i, rate))\n",
    "\n",
    "print(\" \")\n",
    "print(\"Precision by label:\")\n",
    "for i, prec in enumerate(trainingSummary.precisionByLabel):\n",
    "    print(\"label %d: %s\" % (i, prec))\n",
    "\n",
    "print(\" \")\n",
    "print(\"Recall by label:\")\n",
    "for i, rec in enumerate(trainingSummary.recallByLabel):\n",
    "    print(\"label %d: %s\" % (i, rec))\n",
    "\n",
    "print(\" \")\n",
    "print(\"F-measure by label:\")\n",
    "for i, f in enumerate(trainingSummary.fMeasureByLabel()):\n",
    "    print(\"label %d: %s\" % (i, f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ancient-tyler",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Accuracy: 1.0\n",
      "FPR: 0.0\n",
      "TPR: 1.0\n",
      "F-measure: 1.0\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n"
     ]
    }
   ],
   "source": [
    "#Generating confusion matrix and print (includes accuracy)\n",
    "accuracy = trainingSummary.accuracy\n",
    "falsePositiveRate = trainingSummary.weightedFalsePositiveRate\n",
    "truePositiveRate = trainingSummary.weightedTruePositiveRate\n",
    "fMeasure = trainingSummary.weightedFMeasure()\n",
    "precision = trainingSummary.weightedPrecision\n",
    "recall = trainingSummary.weightedRecall\n",
    "print(\" \")\n",
    "print(\"Accuracy: %s\\nFPR: %s\\nTPR: %s\\nF-measure: %s\\nPrecision: %s\\nRecall: %s\"\n",
    "      % (accuracy, falsePositiveRate, truePositiveRate, fMeasure, precision, recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bridal-caribbean",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron Classifier\n",
    "A multilayer perceptron (MLP) is a class of feedforward artificial neural network. It consists of at least three layers of nodes: an input layer, a hidden layer and an output layer. Except for the input nodes, each node is a neuron that uses a nonlinear activation function. MLP utilizes a supervised learning technique called backpropagation for training. Its multiple layers and non-linear activation distinguish MLP from a linear perceptron. It can distinguish data that is not linearly separable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-authentication",
   "metadata": {},
   "source": [
    "#### Building the MLP Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "smaller-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Selecting the features and counting them\n",
    "features = final_data.select(['features']).collect()\n",
    "features_count = len(features[0][0])\n",
    "\n",
    "#Countng number of classes\n",
    "class_count = final_data.select(countDistinct(\"label\")).collect()\n",
    "classes = class_count[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "demanding-firmware",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list of layers\n",
    "#First number in this list is the input layer which has to be equal to the number of features in your vector\n",
    "#Second number is the first hidden layer\n",
    "#Third number is the second hidden layer \n",
    "#Fourth number is the output layer which has to be equal to your class size\n",
    "layers = [features_count, features_count+1, features_count, classes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "indie-green",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the classifier\n",
    "classifier = MultilayerPerceptronClassifier(maxIter=100, layers=layers, blockSize=128, seed=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "soviet-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitiing the model\n",
    "fitModel = classifier.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "adolescent-incidence",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: 683\n"
     ]
    }
   ],
   "source": [
    "#Getting the model weights\n",
    "print(\"Weights:\", fitModel.weights.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "protecting-economics",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generating the predictions on test dataframe\n",
    "predictions = fitModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "isolated-choir",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 90.84745762711864\n"
     ]
    }
   ],
   "source": [
    "accuracy = (MC_eval.evaluate(predictions))*100\n",
    "print(\"Accuracy\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prime-eight",
   "metadata": {},
   "source": [
    "### Naive Bayes\n",
    "The Naive Bayes Classifier is a collection of classification algorithms based on Bayes Theorem. It is not a single algorithm but a family of algorithms that all share a common principle, that every feature being classified is independent of the value of any other feature. \n",
    "\n",
    "**Hyper Parameters:**\n",
    "\n",
    " - **smoothing** = It is problematic when a frequency-based probability is zero, because it will wipe out all the information in the other probabilities, and we need to find a solution for this. A solution would be Laplace smoothing , which is a technique for smoothing categorical data. In PySpark, this number needs to be be >= 0, default is 1.0'.\n",
    " - **thresholds** = Thresholds in multi-class classification to adjust the probability of predicting each class. Array must have length equal to the number of classes, with values > 0, excepting that at most one value may be 0. The class with largest value p/t is predicted, where p is the original probability of that class and t is the class's threshold. The default value is none. \n",
    " - **weightCol** = If you have a weight column you would enter the name of the column here. If this is not set or empty, we treat all instance weights as 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-struggle",
   "metadata": {},
   "source": [
    "#### Building Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "norwegian-arnold",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the classifier\n",
    "classifier = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "revolutionary-monaco",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding the paramters \n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "            .addGrid(classifier.smoothing, [0.0, 0.2, 0.4, 0.6]) \\\n",
    "            .build())\n",
    "#Cross validating \n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                         estimatorParamMaps=paramGrid,\n",
    "                         evaluator=MulticlassClassificationEvaluator(),\n",
    "                         numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "banner-belle",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "fitModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "adjacent-friday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 86.77966101694915\n"
     ]
    }
   ],
   "source": [
    "predictions = fitModel.transform(test)\n",
    "accuracy = (MC_eval.evaluate(predictions))*100\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "therapeutic-worse",
   "metadata": {},
   "source": [
    "### Linear Support Vector Machine\n",
    "Linear SVMs are based on the idea of finding a hyperplane that best divides a dataset into two classes, which is why you can only use it for binary classification. Support vectors are the data points nearest to the hyperplane, the points of a data set that, if removed, would alter the position of the dividing hyperplane. Because of this, they can be considered the critical elements of a data set. Intuitively, the further from the hyperplane our data points lie, the more confident we are that they have been correctly classified. We therefore want our data points to be as far away from the hyperplane as possible, while still being on the correct side of it. So when new testing data is added, whatever side of the hyperplane it lands will decide the class that we assign to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "favorite-nerve",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Counting the number of classes and produce an error if it's more than 2.\n",
    "class_count = final_data.select(countDistinct(\"label\")).collect()\n",
    "classes = class_count[0][0]\n",
    "if classes > 2:\n",
    "    print(\"LinearSVC cannot be used because PySpark currently only accepts binary classification data for this algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "prime-meeting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding parameters\n",
    "classifier = LinearSVC()\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "             .addGrid(classifier.maxIter, [10, 15]) \\\n",
    "             .addGrid(classifier.regParam, [0.1, 0.01]) \\\n",
    "             .build())\n",
    "\n",
    "#Cross Validator requires all of the following parameters:\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "recent-bachelor",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model and getting the best model\n",
    "fitModel = crossval.fit(train)\n",
    "\n",
    "BestModel = fitModel.bestModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "intended-winner",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.118898883442666"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestModel.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "middle-uniform",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([-0.0004, -0.0007, -0.0002, -0.0005, -0.0006, -0.0007, -0.0006, -0.0006, -0.0009, -0.0004, 0.0004, -0.0017, 0.0001, -0.0001, -0.0002, 0.0001, -0.0002])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BestModel.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "convertible-consolidation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  97.96610169491525\n"
     ]
    }
   ],
   "source": [
    "#fit model contains the best model\n",
    "predictions = fitModel.transform(test)\n",
    "accuracy = (MC_eval.evaluate(predictions))*100\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "heard-paste",
   "metadata": {},
   "source": [
    "### Decision Tree Classifier\n",
    "Decision Trees classifiers  are a supervised learning method is used to classify a variable by learning from historical data that the model uses to approximate a sine curve with a set of if-then-else decision rules. The deeper the tree, the more complex the decision rules and the fitter the model. \n",
    "\n",
    "Decision tree builds classification or regression models in the form of a tree structure. It breaks down a data set into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node has two or more branches. Leaf node represents a classification or decision. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.\n",
    "\n",
    "#### Common Hyper Parameters\n",
    "\n",
    " - **maxBins** = Max number of bins for discretizing continuous features. Must be >=2 and >= number of categories for any categorical feature.\n",
    " - **maxDepth** = The max_depth parameter specifies the maximum depth of each tree. The default value for max_depth is None, which means that each tree will expand until every leaf is pure. A pure leaf is one where all of the data on the leaf comes from the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "reliable-respondent",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiating the classifier\n",
    "classifier = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "combined-lucas",
   "metadata": {},
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder() \\\n",
    "#            .addGrid(classifier.maxDepth, [2, 5, 10, 20, 30]) \\\n",
    "             .addGrid(classifier.maxBins, [10, 20, 40, 80, 100]) \\\n",
    "             .build())\n",
    "\n",
    "#Generating cross validator\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "interstate-conference",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "fitModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "legitimate-installation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:  [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "#Collecting and printing feature importances\n",
    "BestModel = fitModel.bestModel\n",
    "featureImportances = BestModel.featureImportances.toArray()\n",
    "print(\"Feature Importances: \",featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "corresponding-stopping",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  100.0\n"
     ]
    }
   ],
   "source": [
    "predictions = fitModel.transform(test)\n",
    "accuracy = (MC_eval.evaluate(predictions))*100\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-orchestra",
   "metadata": {},
   "source": [
    "### Random Forest Classifier\n",
    "Suppose you have a training set with 6 classes, random forest may create three decision trees taking input of each subset. Finally, it predicts based on the majority of votes from each of the decision trees made. This works well because a single decision tree may be prone to noise, but aggregate of many decision trees reduce the effect of noise giving more accurate results. The subsets in different decision trees created may overlap. \n",
    "\n",
    "#### Common Hyper Parameters\n",
    "\n",
    " - **maxBins** = Max number of bins for discretizing continuous features. Must be >=2 and >= number of categories for any categorical feature.\n",
    " - **maxDepth** = The maxDepth parameter specifies the maximum depth of each tree. The default value for max_depth is None, which means that each tree will expand until every leaf is pure. A pure leaf is one where all of the data on the leaf comes from the same class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "three-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "embedded-interval",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding parameter grid\n",
    "paramGrid = (ParamGridBuilder() \\\n",
    "               .addGrid(classifier.maxDepth, [2, 5, 10])\n",
    "#              .addGrid(classifier.maxBins, [5, 10, 20])\n",
    "#              .addGrid(classifier.numTrees, [5, 20, 50])\n",
    "               .build())\n",
    "\n",
    "#Cross validating\n",
    "crossval = CrossValidator(estimator=classifier,\n",
    "                          estimatorParamMaps=paramGrid,\n",
    "                          evaluator=MulticlassClassificationEvaluator(),\n",
    "                          numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "junior-karma",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model\n",
    "fitModel = crossval.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "respiratory-principal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:  [0.         0.         0.         0.00948622 0.06919361 0.10492895\n",
      " 0.1366632  0.02066992 0.10951917 0.         0.         0.54953893\n",
      " 0.         0.         0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "#Retrieving the best model \n",
    "BestModel = fitModel.bestModel\n",
    "featureImportances = BestModel.featureImportances.toArray()\n",
    "print(\"Feature Importances: \",featureImportances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "stylish-mechanism",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = fitModel.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "minus-resolution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      "Accuracy:  100.0\n"
     ]
    }
   ],
   "source": [
    "accuracy = (MC_eval.evaluate(predictions))*100\n",
    "print(\" \")\n",
    "print(\"Accuracy: \",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-notice",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
